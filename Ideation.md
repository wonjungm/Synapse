# Project Synapse: Ideation (v0.2)

## 프로젝트 아이디어 1: 초거대 AI의 경량화를 통한 AI 연구자/개발자 생산성 향상

### 1. 핵심 키워드 (Keywords)
- LLM, Quantization (양자화), Developer Productivity (개발자 생산성)

### 2. 누구를 위해 (For Whom)
- 제한된 GPU 자원(예: 단일 소비자용 GPU)을 가진 AI 연구자, 학생, 혹은 소규모 스타트업의 개발자들을 위해

### 3. 누구의 어떤 문제 해결 위해 (Problem to Solve)
- 수십, 수백 GB에 달하는 최신 LLM을 연구나 서비스 개발에 활용하고 싶지만, 고가의 다중 GPU 서버를 구비하기 어려운 문제. 이로 인해 최신 기술 연구와 아이디어 검증 속도가 저하되는 문제를 해결하고자 함.

### 4. 어떤 기술을 사용해서 (With What Technology)
- **QLoRA, AWQ** 등 최신 양자화 기술을 사용하여, 모델의 핵심 성능은 최대한 유지하면서도 메모리 사용량을 1/4 이하로 줄이는 기술을 적용.
- (추가 탐구) 필요시 Pruning(가지치기) 기술을 결합하여 추가적인 경량화 가능성 탐구.

### 5. 무얼 만들려고 하는가 (What to Build)
- **결과물:** 특정 소형 LLM(예: Llama-3-8B, Gemma-7B)에 최신 양자화 기술을 적용한 **경량화 모델**과 그 **적용 과정을 상세히 기록한 재현 가능한 코드 베이스 및 튜토리얼**.
- **목표:** 일반적인 데스크톱(예: RTX 4090 24GB) 환경에서도 fine-tuning 및 추론이 가능한 수준으로 모델을 최적화하고, 그 성능 저하 폭을 정량적으로 측정하여 발표함.

---

## 프로젝트 아이디어 2: 특정 도메인 맞춤형 소형 모델(SLM) 제작 기술 개발

### 1. 핵심 키워드 (Keywords)
- Knowledge Distillation (지식 증류), SLM (Small Language Model), Domain Adaptation (도메인 적응)

### 2. 누구를 위해 (For Whom)
- 특정 전문 분야(예: 법률, 의료, 금융)에서 AI를 활용한 서비스를 만들고 싶어 하는 중소기업 또는 스타트업.

### 3. 누구의 어떤 문제 해결 위해 (Problem to Solve)
- 거대 범용 LLM은 구독 비용이 비싸고 응답 속도가 느리며, 해당 도메인에 특화되지 않은 일반적인 답변을 생성하는 문제. 저렴하고 빠르면서도, 특정 분야에 전문성을 갖춘 AI 모델이 필요한 문제를 해결하고자 함.

### 4. 어떤 기술을 사용해서 (With What Technology)
- **지식 증류(Knowledge Distillation)** 기술을 핵심으로 사용.
- 거대 LLM(예: GPT-4)을 '교사(Teacher)'로, 소형 LLM(예: Polyglot-ko)을 '학생(Student)'으로 설정하여, 특정 도메인의 데이터셋에 대한 교사의 답변 생성 능력을 학생에게 효율적으로 전수.

### 5. 무얼 만들려고 하는가 (What to Build)
- **결과물:** **'법률 분야 질의응답'** 또는 **'금융 뉴스 요약'** 등 특정 태스크에 고도로 특화된 **소형 언어 모델(SLM)**.
- **목표:** 동일한 태스크에 대해 거대 LLM과 유사한 수준의 성능을 보이면서도, 모델 크기는 1/10 이하, 추론 비용은 수십 분의 1로 절감된 '가성비' 모델을 제작하고 그 효용성을 입증함.
